<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <!-- Part number: Ennnnn-01 -->
    <!-- Published date: Month Year -->
    <!-- Template date: October 15, 2018 -->
    <!-- Put the content ID of this OBE between the empty quotation marks for 'content' below -->
    <meta name="contentid" content="">
    <!-- The title below is displayed on the browser tab, in search results, and in  history. It can be longer than the tutorial title in the <h1> element, below.-->
    <!-- Use an imperative verb in the title. -->
    <title>Lab: Create a highly available NFS service with Oracle Linux 7</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="robots" content="INDEX, FOLLOW">
    <meta name="description" content="Learn how to create a highly available NFS service using Corosync, Ganesha, Pacemaker and Gluster 5.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/linux-labs/css/normalize.min.css">
    <link rel="stylesheet" href="/linux-labs/css/font-awesome.min.css">
    <link rel="stylesheet" href="/linux-labs/css/obe-lite.css">
    <script src="/linux-labs/js/jquery-1.11.0.min.js"></script>
    <script src="/linux-labs/js/jquery-ui-1.10.4.custom.js"></script>
    <script src="/linux-labs/js/jquery.tocify.jd.js"></script>
    <script src="/linux-labs/js/leftnav.js"></script>
  </head>
  <body>
    <header>
      <div class="w1024" style="min-height:55px; display:block;"> <a href="https://docs.oracle.com"

          class="oracle-logo"> <img src="/linux-labs/img/oracle_doc_logo.png" alt="Oracle Documentation"

            style="max-width: none;" height="22" width="236"></a> </div>
    </header>
    <!--end header-->
    <div id="content">
      <!-- The title below is displayed as the title at the top of the OBE page. It can be shorter than the title in the <title> element, above. -->
      <!-- Use an imperative verb in the title. -->
      <h1><img src="/linux-labs/img/obe_tag.png" alt="Oracle by Example branding" align="middle">Creating
        a highly available NFS service with Oracle Linux 7</h1>
      <div class="w1024" style="clear:both; position:relative; margin-top:40px;">
        <div class="navbackwide border-right"><!-- --></div>
        <div id="shownav" title="Show Navigation" tabindex="0"><span class="fa fa-list"></span></div>
        <div id="sidebar"><!-- -->
          <div class="left-nav-tut"><!-- -->
            <div id="hidenavw" title="Hide Navigation" tabindex="0"><span class="fa fa-close"></span></div>
            <div id="navbar" class="left-nav-w"><!-- -->
              <div id="toc" class="tocify"><!-- --></div>
            </div>
          </div>
        </div>
        <div id="bookContainer">
          <article>
            <section>
              <h2><img src="/linux-labs/img/32_begin.png" alt="section 0" class="num_circ" height="32"

                  width="32">Before You Begin</h2>
              <p>This lab will show you how to install and configure a highly
                available NFS service on Oracle Linux 7 using Corosync, Pacemaker,
                Gluster and Ganesha.</p>
              <h3>Background</h3>
              <p>In this lab we will create a NFS service hosted by three VMs:
                <strong>master1</strong>, <strong>master2</strong> and <strong>master3</strong>. Each of these VMs will replicate a
                Gluster volume for data redundancy and use clustering tools for
                service redundancy.</p>
              <p>A fourth VM named <strong>client1</strong> will mount this NFS service for
                demonstration and testing.</p>
              <h4>Components<br>
              </h4>
              <ul>
                <li><strong>Corosync</strong> provides clustering infrastructure to manage which
                  nodes are involved, their communication and quorum</li>
                <li><strong>Pacemaker</strong> manages cluster resources and rules of their behavior</li>
                <li><strong>Gluster</strong> is a scalable and distributed filesystem</li>
                <li><strong>Ganesha</strong> is an NFS server which can use many different
                  backing filesystem types including Gluster</li>
                
                
              </ul>
              <h3>What Do You Need?</h3>
              <p>If you're attending the Hands-On Lab at Oracle OpenWorld 2019,
                your laptop already has the required software installed.</p>
              <p>If you're following this from home then you will first need to
                install:</p>
              <ul>
                <li><a href="https://www.vagrantup.com/">Vagrant by HashiCorp</a></li>
                <li><a href="https://www.virtualbox.org/">Oracle VM Virtualbox</a></li>
              </ul>
              <h3>Environment</h3>
              <figure><img src="files/lab-network.svg" alt="Description of lab-network.svg follows"

                  style="width: 587px; height: 262px;"> <figcaption><a href="files/lab-network.txt">Diagram
                    of our lab [lab-network.svg]</a><br>
                </figcaption> </figure>
              <table summary="This table describes network addressing of our lab environment">
                <tbody>
                  <tr>
                    <th scope="col">Hostname</th>
                    <th scope="col">IP Address</th>
                    <th scope="col">Fully Qualified Hostname</th>
                  </tr>
                  <tr>
                    <td scope="row">master1</td>
                    <td>192.168.99.101</td>
                    <td>master1.vagrant.vm</td>
                  </tr>
                  <tr>
                    <td scope="row">master2</td>
                    <td>192.168.99.102</td>
                    <td>master2.vagrant.vm</td>
                  </tr>
                  <tr>
                    <td scope="row">master3</td>
                    <td>192.168.99.103</td>
                    <td>master3.vagrant.vm</td>
                  </tr>
                  <tr>
                    <td scope="row">client1</td>
                    <td>192.168.99.104</td>
                    <td>client1.vagrant.vm</td>
                  </tr>
                  <tr>
                    <td scope="row">nfs</td>
                    <td>192.168.99.100</td>
                    <td>nfs.vagrant.vm</td>
                  </tr>
                </tbody>
              </table>
              <p>This lab involves multiple VMs and you will need to perform
                different steps on different VMs. The easiest way to do this is
                through the vagrant command</p>
              <p><code>vagrant ssh &lt;hostname&gt;</code></p>
              <p>Once connected you can return to your desktop with a standard <code>
                  logout</code> or <code>exit</code> command.</p>
              <p> For example </p>
              <ol>
                <pre>[user@demo lab]$ <strong>vagrant ssh master1</strong>
Last login: Tue Aug 20 23:56:58 2019 from 10.0.2.2
[vagrant@master1 ~]$ <strong>hostname</strong>
master1.vagrant.vm
[vagrant@master1 ~]$ <strong>exit</strong>
logout
Connection to 127.0.0.1 closed.
[user@demo lab]$ <strong>vagrant ssh master3</strong>
Last login: Tue Aug 20 05:25:49 2019 from 10.0.2.2
[vagrant@master3 ~]$ <strong>hostname</strong>
master3.vagrant.vm
[vagrant@master3 ~]$ <strong>logout</strong>
Connection to 127.0.0.1 closed.</pre>
              </ol>
              <p>All steps are executed as the user <code>root</code> so you
                will need to change to <code>root</code> user using <code>sudo
                  -i</code></p>
              <ol>
                <pre>[user@demo lab]$ <strong>vagrant ssh master1</strong><br>Last login: Tue Aug 20 23:56:58 2019 from 10.0.2.2<br>[vagrant@master1 ~]$ <strong>sudo -i</strong>
[root@master1 ~]# <strong>exit</strong>
logout
[vagrant@master1 ~]$ <strong>exit</strong>
logout
Connection to 127.0.0.1 closed.
</pre>
              </ol>
              <p>To avoid continually logging in and out you may want to open
                four terminal windows, one for each of <code>master1, master2, master3</code> and <code>client1</code> to easily switch between VMs. If you are doing this, ensure that
                all your terminals are in the in the same directory so the <code>vagrant
                  ssh</code> commands succeed.</p>
              <p> A step may say "On all masters..." and should be performed on
                <code>master1, master2 and master3</code>. This is done to remove redundancy as the action and result are identical.</p>
            </section>
            <!-- ========================================================================================================================= -->
            <section>
              <hr>
              <h2 id="section_1" role="button" style="text-align: left;"><img src="/linux-labs/img/32_1.png"

                  alt="section 1" class="num_circ" height="32" width="32">Start
                the lab environment</h2>
              <p>You will first download and start the VMs we will be using in
                this lab environment. This is simplified through the use of
                Vagrant.</p>
              <p>If you're at Oracle OpenWorld 2019 you can skip this step and move
                onto <em>Install Software</em>.</p>
              <ol>
                <li>Download the lab configuration
                  <pre># <strong>git clone https://github.com/oracle/linux-labs.git</strong></pre>
                </li>
                <li>Change to the lab directory
                  <pre># <strong>cd linux-labs/HA-NFS/</strong></pre>
                </li>
                <li>Start the lab virtual machines
                  <pre># <strong>vagrant up</strong></pre>
                </li>
              </ol>
              <p>Vagrant will download an Oracle Linux image and create four
                virtual machines with unique networking configuration. A fifth
                IP address is used for our shared NFS service.</p>
              <p>Remember you can access them using the <code>vagrant ssh
                  hostname</code> command mentioned above and you do not need to
                connect via address. </p>
            </section>
            <section>
              <hr>
              <h2 id="section_2" role="button" style="text-align: left;"><img src="/linux-labs/img/32_2.png"

                  alt="section 2" class="num_circ" height="32" width="32">Install
                software</h2>
              <p>Now enable the required Oracle Linux repositories before
                installing the Corosync, Ganesha, Gluster and Pacemaker software.</p>
                <p>Remember to use <code><strong>vagrant ssh hostname</strong></</strong></code> to login to each of the masters and <code><strong>sudo -i</strong></code> to elevate your privileges to root <em>before</em> running these commands. </p>
              <ol>
                <li>(On <strong>all </strong>masters) Install the Gluster yum
                  repository configuration
                  <pre># <strong>yum install -y oracle-gluster-release-el7</strong></pre>
                </li>
                <li>(On <strong>all</strong> masters) Enable the repositories
                  <pre># <strong>yum-config-manager --enable ol7_addons \
                              ol7_latest \
                              ol7_optional_latest \
                              ol7_UEKR5</strong></pre>
                </li>
                <li>(On <strong>all</strong> masters) Install the software
                  <pre># <strong>yum install -y corosync \
                 glusterfs-server \
                 nfs-ganesha-gluster \
                 pacemaker \
                 pcs </strong></pre>
                </li>
              </ol>
            </section>
            <hr>
            <section>
              <h2 id="section_3" role="button" style="text-align: left;"><img src="/linux-labs/img/32_3.png"

                  alt="section 3" class="num_circ" height="32" width="32">Create
                the Gluster volume</h2>
              <p>You will prepare each VMs disk, create a replicated Gluster
                volume and activate the volume</p>
              <ol>
                <li>(On <strong>all</strong> masters) Create an XFS filesystem
                  on <code>/dev/sdb</code> with a label of <code>gluster-000</code>
                  <pre># <strong>mkfs.xfs -f -i size=512 -L gluster-000 /dev/sdb</strong></pre>
                </li>
                <li>(On <strong>all</strong> masters) Create a mountpoint, add
                  an fstab(5) entry for a disk with the label <code>gluster-000</code>
                  and mount the filesystem
                  <pre># <strong>mkdir -p /data/glusterfs/sharedvol/mybrick</strong>
# <strong>echo 'LABEL=gluster-000 /data/glusterfs/sharedvol/mybrick xfs defaults  0 0' &gt;&gt; /etc/fstab</strong>
# <strong>mount /data/glusterfs/sharedvol/mybrick</strong></pre>
                </li>
                <li>(On <strong>all</strong> masters) Enable and start the
                  Gluster service
                  <pre># <strong>systemctl enable --now glusterd</strong>
<strong></strong></pre> </li>
                <li>On master1: Create the Gluster environment by adding peers
                  <pre># <strong>gluster peer probe master2.vagrant.vm</strong>
peer probe: success. 
# <strong>gluster peer probe master3.vagrant.vm</strong>
peer probe: success. 
# <strong>gluster peer status</strong>
Number of Peers: 2

Hostname: master2.vagrant.vm
Uuid: 328b1652-c69a-46ee-b4e6-4290aef11043
State: Peer in Cluster (Connected)

Hostname: master3.vagrant.vm
Uuid: 384aa447-4e66-480a-9293-8c10218395a4
State: Peer in Cluster (Connected)
<strong></strong></pre> </li>
                <li>Show that our peers have joined the environment
                  <p> On master2:</p>
                  <pre># <strong>gluster peer status</strong>
Number of Peers: 2

Hostname: master3.vagrant.vm
Uuid: 384aa447-4e66-480a-9293-8c10218395a4
State: Peer in Cluster (Connected)

Hostname: master1.vagrant.vm
Uuid: ac64c0e3-02f6-4814-83ca-1983999c2bdc
State: Peer in Cluster (Connected)
</pre>
                  <p>On master3:</p>
                  <pre># <strong>gluster peer status</strong>
Number of Peers: 2

Hostname: master1.vagrant.vm
Uuid: ac64c0e3-02f6-4814-83ca-1983999c2bdc
State: Peer in Cluster (Connected)

Hostname: master2.vagrant.vm
Uuid: 328b1652-c69a-46ee-b4e6-4290aef11043
State: Peer in Cluster (Connected)</pre>
                </li>
                <li>On master1: Create a Gluster volume named <code>sharedvol</code>
                  which is replicated across our three hosts: master1, master2
                  and master3.<br>
                  <pre># <strong>gluster volume create sharedvol \
                        replica 3 \
                        master{1,2,3}:/data/glusterfs/sharedvol/mybrick/brick</strong></pre>
                  For more details on volume types see the <em>Gluster: Setting
                    up Volumes</em> link in the additional information section
                  of this page</li>
                <li>On master1: Enable our Gluster volume named <code>sharedvol</code>
                  <pre># <strong>gluster volume start sharedvol</strong></pre>
                </li>
              </ol>
              <p> Our replicated Gluster volume is now available and can be
                verified from any master</p>
              <ol>
                <pre># <strong>gluster volume info</strong>
Volume Name: sharedvol
Type: Replicate
Volume ID: 466a6c8e-7764-4c0f-bfe6-591cc6a570e8
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: master1:/data/glusterfs/sharedvol/mybrick/brick
Brick2: master2:/data/glusterfs/sharedvol/mybrick/brick
Brick3: master3:/data/glusterfs/sharedvol/mybrick/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
</pre>
                <pre># <strong>gluster volume status</strong>
Status of volume: sharedvol
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick master1:/data/glusterfs/sharedvol/myb
rick/brick                                  49152     0          Y       7098 
Brick master2:/data/glusterfs/sharedvol/myb
rick/brick                                  49152     0          Y       6860 
Brick master3:/data/glusterfs/sharedvol/myb
rick/brick                                  49152     0          Y       6440 
Self-heal Daemon on localhost               N/A       N/A        Y       7448 
Self-heal Daemon on master3.vagrant.vm      N/A       N/A        Y       16839
Self-heal Daemon on master2.vagrant.vm      N/A       N/A        Y       7137 
 
Task Status of Volume sharedvol
------------------------------------------------------------------------------
There are no active volume tasks</pre>
              </ol>
            </section>
            <hr>
            <section>
              <h2 id="section_4" role="button" style="text-align: left;"><img src="/linux-labs/img/32_4.png"

                  alt="section 4" class="num_circ" height="32" width="32">Configure
                Ganesha</h2>
              <p> Ganesha is the NFS server that shares out the Gluster volume.
                In this example we allow any NFS client to connect to our NFS
                share with read/write permissions. </p>
              <ol>
                (On <strong>all</strong> masters) Populate the file <code>/etc/ganesha/ganesha.conf</code>
                with our configuration
                <pre># <strong>cat &lt;&lt; EOF &gt; /etc/ganesha/ganesha.conf</strong>
EXPORT{
    Export_Id = 1 ;       # Unique identifier for each EXPORT (share)
    Path = "/sharedvol";  # Export path of our NFS share

    FSAL {
        name = GLUSTER;          # Backing type is Gluster
        hostname = "localhost";  # Hostname of Gluster server
        volume = "sharedvol";    # The name of our Gluster volume
    }

    Access_type = RW;          # Export access permissions
    Squash = No_root_squash;   # Control NFS root squashing
    Disable_ACL = FALSE;       # Enable NFSv4 ACLs
    Pseudo = "/sharedvol";     # NFSv4 pseudo path for our NFS share
    Protocols = "3","4" ;      # NFS protocols supported
    Transports = "UDP","TCP" ; # Transport protocols supported
    SecType = "sys";           # NFS Security flavors supported
}<br><strong>EOF</strong></pre>
              </ol>
              <p>For more options to control permissions see the <code>EXPORT
                  {CLIENT{}}</code> section of<em> config_samples-export </em>in
                the <em>Additional Information</em> section of this page</p>
            </section>
            <hr>
            <section>
              <h2 id="section_5" role="button" style="text-align: left;"><img src="/linux-labs/img/32_5.png"

                  alt="section 5" class="num_circ" height="32" width="32">Create
                a Cluster</h2>
              <p> You will create and start a Pacemaker/Corosync cluster made of
                our three master nodes </p>
              <ol>
                <li>(On <strong>all</strong> masters) Set a shared password of
                  your choice for the user <code>hacluster</code>
                  <pre># <strong>passwd hacluster</strong>
Changing password for user hacluster.
New password: <var>examplepassword</var>
Retype new password: <var>examplepassword</var></pre>
                </li>
                <li>(On <strong>all</strong> masters) Enable the Corosync and
                  Pacemaker services. Enable and start the configuration system
                  service<br>
                  <pre># <strong>systemctl enable corosync</strong>
# <strong>systemctl enable pacemaker</strong>
# <strong>systemctl enable --now pcsd</strong></pre>
                </li>
                <ul>
                  Note we did not start the Corosync and Pacemaker services,
                  this happens later in step five
                </ul>
                <li>On master1: Authenticate with all cluster nodes using the <code>hacluster</code>
                  user and password defined above
                  <pre># <strong>pcs cluster auth master1 master2 master3 \
                   -u hacluster -p </strong><var>examplepassword</var>
master1: Authorized
master3: Authorized
master2: Authorized</pre>
                </li>
                <li>On master1: Create a cluster named <code>HA-NFS </code>
                  <pre># <strong>pcs cluster setup --name HA-NFS master1 master2 master3</strong>
Destroying cluster on nodes: master1, master2, master3...
master1: Stopping Cluster (pacemaker)...
master2: Stopping Cluster (pacemaker)...
master3: Stopping Cluster (pacemaker)...
master2: Successfully destroyed cluster
master1: Successfully destroyed cluster
master3: Successfully destroyed cluster

Sending 'pacemaker_remote authkey' to 'master1', 'master2', 'master3'
master1: successful distribution of the file 'pacemaker_remote authkey'
master2: successful distribution of the file 'pacemaker_remote authkey'
master3: successful distribution of the file 'pacemaker_remote authkey'
Sending cluster config files to the nodes...
master1: Succeeded
master2: Succeeded
master3: Succeeded

Synchronizing pcsd certificates on nodes master1, master2, master3...
master1: Success
master3: Success
master2: Success
Restarting pcsd on the nodes in order to reload the certificates...
master1: Success
master3: Success
master2: Success</pre>
                </li>
                <li>On master1: Start the cluster on all nodes
                  <pre># <strong>pcs cluster start --all</strong>
master1: Starting Cluster (corosync)...
master2: Starting Cluster (corosync)...
master3: Starting Cluster (corosync)...
master1: Starting Cluster (pacemaker)...
master3: Starting Cluster (pacemaker)...
master2: Starting Cluster (pacemaker)...</pre>
                </li>
                <li>On master1: Enable the cluster to run on all nodes at boot
                  time
                  <pre># <strong>pcs cluster enable --all</strong>
master1: Cluster Enabled
master2: Cluster Enabled
master3: Cluster Enabled</pre>
                </li>
                <li>On master1: Disable STONITH
                  <pre># <strong>pcs property set stonith-enabled=false</strong></pre>
                </li>
              </ol>
              <p> Our cluster is now running</p>
              <p>On any master:</p>
              <ol>
                <pre># <strong>pcs cluster status</strong>
Cluster Status:
 Stack: corosync
 Current DC: master2 (version 1.1.20-5.el7-3c4c782f70) - partition with quorum
 Last updated: Wed Aug 21 03:37:15 2019
 Last change: Wed Aug 21 03:34:48 2019 by root via cibadmin on master1
 3 nodes configured
 0 resources configured

PCSD Status:
  master3: Online
  master2: Online
  master1: Online</pre>
              </ol>
            </section>
            <hr>
            <section>
              <h2 id="section_6" role="button" style="text-align: left;"><img src="/linux-labs/img/32_6.png"

                  alt="section 6" class="num_circ" height="32" width="32">Create
                Cluster services</h2>
              <p> You will create a Pacemaker resource group that contains the
                resources necessary to host NFS services from the hostname
                nfs.vagrant.vm (192.168.99.100)</p>
              <ol>
                <li>On master1: Create a systemd based cluster resource to
                  ensure <code>nfs-ganesha</code> is running
                  <pre># <strong>pcs resource create nfs_server \
                      systemd:nfs-ganesha \
                      op monitor interval=10s</strong></pre>
                </li>
                <li>On master1: Create a IP cluster resource used to present the
                  NFS server
                  <pre># <strong>pcs resource create nfs_ip \
                      ocf:heartbeat:IPaddr2 \
                      ip=192.168.99.100 \
                      cidr_netmask=24 \
                      op monitor interval=10s</strong></pre>
                </li>
                <li>On master1: Join the Ganesha service and IP resource in a
                  group to ensure they remain together on the same host
                  <pre># <strong>pcs resource group add nfs_group nfs_server nfs_ip</strong></pre>
                </li>
              </ol>
              <p>Our service is now running</p>
              <ol>
                <pre># <strong>pcs status</strong>
Cluster name: HA-NFS
Stack: corosync
Current DC: master2 (version 1.1.20-5.el7-3c4c782f70) - partition with quorum
Last updated: Wed Aug 21 03:45:46 2019
Last change: Wed Aug 21 03:45:40 2019 by root via cibadmin on master1

3 nodes configured
2 resources configured

Online: [ master1 master2 master3 ]

Full list of resources:

 Resource Group: nfs_group
     nfs_server	(systemd:nfs-ganesha):	Started master1
     nfs_ip	(ocf::heartbeat:IPaddr2):	Started master1

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled</pre>
              </ol>
            </section>
            <hr>
            <section>
              <h2 role="button" style="text-align: left;" id="section_7"><img src="/linux-labs/img/32_7.png"

                  alt="section 7" class="num_circ" height="32" width="32">Test
                NFS availability using a client</h2>
              <p>You may want to have two terminal windows open for these steps
                as we test failover with master1 and client1</p>
              <ol>
                <li>On client1: Mount the NFS service provided by our cluster
                  and create a file<br>
                  <pre># <strong>yum install -y nfs-utils</strong>
# <strong>mkdir /sharedvol</strong>
#<strong> mount -t nfs nfs.vagrant.vm:/sharedvol /sharedvol</strong>
# <strong>df -h /sharedvol/</strong>
Filesystem                 Size  Used Avail Use% Mounted on
nfs.vagrant.vm:/sharedvol   16G  192M   16G   2% /sharedvol
# <strong>echo "Hello from OpenWorld" &gt; /sharedvol/hello</strong></pre>
                </li>
                <li>On master1: Identify the host running the <code>nfs_group</code>
                  resources and put it in standby mode to stop running services
                  <pre># <strong>pcs status</strong>
Cluster name: HA-NFS
Stack: corosync
Current DC: master2 (version 1.1.20-5.el7-3c4c782f70) - partition with quorum
Last updated: Wed Aug 21 03:45:46 2019
Last change: Wed Aug 21 03:45:40 2019 by root via cibadmin on master1

3 nodes configured
2 resources configured

Online: [ master1 master2 master3 ]

Full list of resources:

 Resource Group: nfs_group
     nfs_server	(systemd:nfs-ganesha):	<strong>Started master1</strong>
     nfs_ip	(ocf::heartbeat:IPaddr2):	<strong>Started master1</strong>

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
# <strong>pcs node standby master1</strong></pre>
                </li>
                <li>On master1: Verify that the <code>nfs_group</code>
                  resources have moved to another node
                  <pre># <strong>pcs status</strong>
Cluster name: HA-NFS
Stack: corosync
Current DC: master2 (version 1.1.20-5.el7-3c4c782f70) - partition with quorum
Last updated: Wed Aug 21 04:02:46 2019
Last change: Wed Aug 21 04:01:51 2019 by root via cibadmin on master1

3 nodes configured
2 resources configured

Node master1: standby
Online: [ master2 master3 ]

Full list of resources:

 Resource Group: nfs_group
     nfs_server	(systemd:nfs-ganesha):	<strong>Started master2</strong>
     nfs_ip	(ocf::heartbeat:IPaddr2):	<strong>Started master2</strong>

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled</pre>
                </li>
                <li>On client1: Verify your file is still accessible
                  <p>This will have a short delay as the service moves from one
                    master to another</p>
                  <pre># <strong>ls -la /sharedvol/</strong>
total 9
drwxr-xr-x.  3 root root 4096 Aug 21 03:59 .
dr-xr-xr-x. 20 root root 4096 Aug 21 02:57 ..
-rw-r--r--.  1 root root   21 Aug 21 03:59 hello
# <strong>cat /sharedvol/hello</strong>
Hello from OpenWorld
</pre></li>
                <li>On master1: Bring our standby node back into the cluster
                  <pre># <strong>pcs node unstandby master1</strong></pre>
                </li>
              </ol>
              <p>You now have an understanding of how you could use
                Pacemaker/Corosync to create highly available services backed by
                Gluster.</p>
            </section>
            <hr>
            <section>
              <h2 id="section_8" role="button" style="text-align: left;"><img src="/linux-labs/img/32_8.png"

                  alt="section 8" class="num_circ" height="32" width="32">Enable
                Gluster encryption (Optional)</h2>
              <p>You will create a self signed certificate for each master and
                have it be trusted by its peers. </p>
              <p>For more options see<em> Setting up Transport Layer Security</em>
                in the <em>Additional Information</em> section of this page</p>
              <ol>
                <li>(On <strong>all</strong> masters) Create a private key and
                  then create a certificate for this host signed with this key
                  <pre># <strong>openssl genrsa -out /etc/ssl/glusterfs.key 2048</strong>
# <strong>openssl req -new -x509 -days 365 -key /etc/ssl/glusterfs.key \
                                   -out /etc/ssl/glusterfs.pem \
                                   -subj "/CN=${HOSTNAME}/"</strong></pre>
                </li>
                <li>(On <strong>all</strong> masters) Combine the certificate
                  from each node into one file all masters can trust
                  <pre># <strong>cat /etc/ssl/glusterfs.pem &gt;&gt; /vagrant/combined.ca.pem</strong></pre>
                </li>
                <li>(On <strong>all</strong> masters) Copy the combined list of
                  trusted certificates to the local system for Gluster use
                  <pre># <strong>cp /vagrant/combined.ca.pem /etc/ssl/glusterfs.ca</strong></pre>
                </li>
                <li> (On <strong>all</strong> masters) Enable encryption for
                  Gluster management traffic
                  <pre># <strong>touch /var/lib/glusterd/secure-access</strong></pre>
                </li>
                <li>On master1: Enable encryption on the Gluster volume <code>sharedvol</code>
                  <pre># <strong>gluster volume set sharedvol client.ssl on</strong>
# <strong>gluster volume set sharedvol server.ssl on</strong>
</pre></li>
                <li> (On <strong>all</strong> masters) Restart the Gluster
                  service
                  <pre># <strong>systemctl restart glusterd</strong></pre>
                </li>
              </ol>
              <p> Our Gluster volume now has transport encryption enabled</p>
              <ol>
                <pre># <strong>gluster volume info</strong>
Volume Name: sharedvol
Type: Replicate
Volume ID: 970effb5-5d9a-4ece-9188-7f0525010acf
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: master1:/data/glusterfs/sharedvol/mybrick/brick
Brick2: master2:/data/glusterfs/sharedvol/mybrick/brick
Brick3: master3:/data/glusterfs/sharedvol/mybrick/brick
Options Reconfigured:
server.ssl: <strong>on</strong>
client.ssl: <strong>on</strong>
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
</pre>
              </ol>
            </section>
            <hr>
            <section>
              <!-- Include the More Information section only in a standalone tutorial or in the final lab or tutorial in a series (not in a learning path) -->
              <h2 id="more_info" role="button" style="text-align: left;"><img src="/linux-labs/img/32_more.png"

                  alt="more information" class="num_circ" height="32" width="32">Want
                to Learn More?</h2>
              <ul>
                <li><a href="https://docs.oracle.com/cd/E52668_01/F18418/html/gluster-using.html#gluster-volumes"

                    target="_blank">Gluster: Setting up Volumes</a></li>
                <li><a href="https://docs.oracle.com/cd/E52668_01/F18418/html/gluster-tls.html"

                    target="_blank">Gluster: Setting up Transport Layer Security</a></li>
                <li><a href="https://github.com/nfs-ganesha/nfs-ganesha/blob/next/src/config_samples/export.txt"

                    target="_blank">Ganesha: config_samples-export</a></li>
              </ul>
            </section>
            <hr> </article>
        </div>
        <br class="clearfloat">
      </div>
    </div>
    <!--close main-->
    <!--end content-->
    <div class="footer-container ">
      <div style="max-width: 994px; padding:10px 0 0 17px;">
        <footer class="footer-list">
          <ul>
            <li> <a href="https://www.oracle.com/corporate/index.html" target="_blank">About
                Oracle</a> </li>
            <li> <a href="https://www.oracle.com/us/corporate/contact/index.html"

                target="_blank">Contact Us</a> </li>
            <li> <a href="https://www.oracle.com/us/legal/index.html" target="_blank">Legal
                Notices</a> </li>
            <li> <a href="https://www.oracle.com/us/legal/terms/index.html" target="_blank">Terms
                of Use</a> </li>
            <li> <a href="https://www.oracle.com/us/legal/privacy/index.html" target="_blank">Your
                Privacy Rights</a> </li>
            <li> <a href="https://www.oracle.com/pls/topic/lookup?ctx=cpyr&amp;id=en">Copyright
                © 2019, Oracle and/or its affiliates. All rights reserved.</a></li>
          </ul>
        </footer>
      </div>
      <script type="text/javascript" src="https://www.oracleimg.com/us/assets/metrics/ora_docs.js"></script>
    </div>
    <!-- OBE Dialog Box code 10/15/2018 -->
    <!-- DO NOT ALTER THE CODE BELOW   -->
    <div id="lpModal" class="modal">
      <!-- Modal hdr and body -->
      <div id="obe_dialog" class="modal-content">
        <div class="modal-header"> <a href="#" class="close"> <span class="close">×</span>
          </a>
          <h1>Associated Learning Paths</h1>
        </div>
        <div id="dialog-content"> </div>
        <div id="dialog-close" class="closeBtn"> <a href="#" class="close2">
            <p class="close2"> <span>&nbsp;&nbsp;View&nbsp;&nbsp;</span> </p>
          </a> </div>
      </div>
    </div>
    <!-- END OBE Dialog Box code -->
  </body>
</html>
